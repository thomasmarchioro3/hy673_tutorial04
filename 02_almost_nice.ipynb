{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.distributions import Normal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CouplingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28  # input size (MNIST)\n",
    "hidden_dim = 1000  # output size of the hidden layers\n",
    "num_coupling_layers = 5  # number of coupling layers\n",
    "num_layers = 6  # number of linear layers for each coupling layer\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling layer\n",
    "\n",
    "The scaling layer just applies a multiplicative factor to the (one different factor for each component):\n",
    "\n",
    "\\begin{equation*}\n",
    "    y = \\exp(-\\theta) \\odot x\n",
    "\\end{equation*}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta = \\theta_1, \\dots, \\theta_d\n",
    "\\end{equation*}\n",
    "\n",
    "<b>Why the exp?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalingLayer(nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super().__init__()\n",
    "    self.log_scale_vector = nn.Parameter(torch.randn(1, input_dim, requires_grad=True))\n",
    "\n",
    "  def forward(self, x):\n",
    "    log_det_jacobian = torch.sum(self.log_scale_vector)\n",
    "    log_likelihood = torch.exp(self.log_scale_vector) * x\n",
    "    return log_likelihood, log_det_jacobian\n",
    "  \n",
    "  def inverse(self, x):\n",
    "    return torch.exp(- self.log_scale_vector) * x  # we do not need the jacobian for the inverse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlmonstNICE\n",
    "\n",
    "- Implements scaling layer at the end\n",
    "- Uses Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlmostNICE(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim=1000, num_coupling_layers=3, num_layers=6, device='cpu'):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_coupling_layers = num_coupling_layers\n",
    "    self.num_layers = num_layers  # number of linear layers for each coupling layer\n",
    "\n",
    "    # alternating mask orientations for consecutive coupling layers\n",
    "    masks = [self._get_mask(input_dim, orientation=(i % 2 == 0)).to(device)\n",
    "                                            for i in range(num_coupling_layers)]\n",
    "\n",
    "    self.coupling_layers = nn.ModuleList([CouplingLayer(input_dim=input_dim,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                mask=masks[i], num_layers=num_layers)\n",
    "                              for i in range(num_coupling_layers)])\n",
    "\n",
    "    self.scaling_layer = ScalingLayer(input_dim=input_dim)\n",
    "\n",
    "    self.prior = Normal(0, 1)\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    z = x\n",
    "    for i in range(len(self.coupling_layers)):  # pass through each coupling layer\n",
    "      z = self.coupling_layers[i](z)\n",
    "    z, log_det_jacobian = self.scaling_layer(z)\n",
    "\n",
    "    log_likelihood = torch.sum(self.prior.log_prob(z), dim=1) + log_det_jacobian\n",
    "\n",
    "    return z, log_likelihood\n",
    "\n",
    "  def inverse(self, z):\n",
    "    x = z\n",
    "    x = self.scaling_layer.inverse(x)\n",
    "    for i in reversed(range(len(self.coupling_layers))):  # pass through each coupling layer in reversed order\n",
    "      x = self.coupling_layers[i].inverse(x)\n",
    "    return x\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    z = self.prior.sample([num_samples, self.input_dim]).view(num_samples, self.input_dim)\n",
    "    z = z.to(self.device)\n",
    "    return self.inverse(z)\n",
    "\n",
    "  def _get_mask(self, dim, orientation=True):\n",
    "    mask = torch.zeros(dim)\n",
    "    mask[::2] = 1.\n",
    "    if orientation:\n",
    "      mask = 1. - mask # flip mask if orientation is True\n",
    "    return mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and data loader\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NICE model\n",
    "model = AlmostNICE(input_dim=input_dim, num_coupling_layers=num_coupling_layers, num_layers=num_layers, device=device).to(device)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  tot_log_likelihood = 0\n",
    "  batch_counter = 0\n",
    "\n",
    "  for batch_id, (x, _) in tqdm(enumerate(train_loader)):\n",
    "      \n",
    "      model.zero_grad()\n",
    "\n",
    "      x = x.to(device)\n",
    "      x = x.view(-1, 28*28)  # flatten\n",
    "      \n",
    "      z, log_likelihood = model(x)\n",
    "      loss = -torch.mean(log_likelihood)  # NLL\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()      \n",
    "\n",
    "      tot_log_likelihood -= loss\n",
    "      batch_counter += 1\n",
    "\n",
    "  mean_log_likelihood = tot_log_likelihood / batch_counter  # normalize w.r.t. the batches\n",
    "  print(f'Epoch {epoch+1:d} completed. Log Likelihood: {mean_log_likelihood:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.isdir(\"saved_models\"):\n",
    "#     os.makedirs(\"saved_models\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"saved_models/AlmostNICE.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
