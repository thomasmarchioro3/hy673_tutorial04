{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.distributions import Normal  # using torch distributions\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28  # input size (MNIST)\n",
    "hidden_dim = 1000  # output size of the hidden layers\n",
    "num_coupling_layers = 5  # number of coupling layers\n",
    "num_layers = 6  # number of linear layers for each coupling layer\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(20)\n",
    "mask[::2] = 1\n",
    "mask = mask.float()\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x * mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x * (1 - mask))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = Normal(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = distribution.sample((20,))\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution.log_prob(z)  # log-likelihoods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Coupling\n",
    "\n",
    "Recap of additive coupling.\n",
    "\n",
    "We split the input $x$ in two equal parts $x_1$ and $x_2$.\n",
    "\n",
    "A coupling layer transforms only $x_2$ based on $x_1$ (or vice versa).\n",
    "\n",
    "\\begin{equation*}\n",
    "    y_1 = x_1, \\ \\ \\ \\ y_2 = x_2 + m_{\\theta}(x_1), \\ \\ \\ \\ y = \\text{concat}(y_1, y_2)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, hidden_dim, mask, num_layers=4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mask = mask\n",
    "\n",
    "    modules = [nn.Linear(input_dim, hidden_dim), \n",
    "               nn.LeakyReLU(0.2)]\n",
    "    \n",
    "    for _ in range(num_layers - 2):\n",
    "      modules.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "      modules.append(nn.LeakyReLU(0.2))\n",
    "    modules.append(nn.Linear(hidden_dim, input_dim))\n",
    "\n",
    "    self.m = nn.Sequential(*modules)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x1 = self.mask * x\n",
    "      x2 = (1 - self.mask) * x\n",
    "      y1 = x1\n",
    "      y2 = x2 + (self.m(x1) * (1 - self.mask))\n",
    "      return y1 + y2\n",
    "    \n",
    "  # inverse mapping\n",
    "  def inverse(self, x):\n",
    "    y1 = self.mask * x\n",
    "    y2 =(1 - self.mask) * x\n",
    "    x1 = y1\n",
    "    x2 = y2 - (self.m(y1) * (1 - self.mask))\n",
    "    return x1 + x2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not-So-NICE (Gaussian prior + No scaling)\n",
    "\n",
    "This architecture consists just in a sequence of coupling layers, with alternating masks:\n",
    "\n",
    "- half of the masks cover even indices, the other half covers odd indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSoNICE(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim=1000, num_coupling_layers=3, num_layers=6, device='cpu'):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_coupling_layers = num_coupling_layers\n",
    "    self.num_layers = num_layers  # number of linear layers for each coupling layer\n",
    "\n",
    "    # alternating mask orientations for consecutive coupling layers\n",
    "    masks = [self._get_mask(input_dim, orientation=(i % 2 == 0)).to(device)\n",
    "                                            for i in range(num_coupling_layers)]\n",
    "\n",
    "    self.coupling_layers = nn.ModuleList([CouplingLayer(input_dim=input_dim,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                mask=masks[i], num_layers=num_layers)\n",
    "                              for i in range(num_coupling_layers)])\n",
    "\n",
    "    self.prior = Normal(0, 1)\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    z = x\n",
    "    for i in range(len(self.coupling_layers)):  # pass through each coupling layer\n",
    "      z = self.coupling_layers[i](z)\n",
    "\n",
    "    log_likelihood = torch.sum(self.prior.log_prob(z), dim=1)\n",
    "    return z, log_likelihood\n",
    "\n",
    "  # we don't call this during training, but we use it for inference\n",
    "  def inverse(self, z):\n",
    "    x = z\n",
    "    for i in reversed(range(len(self.coupling_layers))):  # pass through each coupling layer in reversed order\n",
    "      x = self.coupling_layers[i].inverse(x)\n",
    "    return x\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    z = self.prior.sample([num_samples, self.input_dim]).view(num_samples, self.input_dim)\n",
    "    return self.inverse(z)\n",
    "\n",
    "  def _get_mask(self, dim, orientation=True):\n",
    "    mask = torch.zeros(dim)\n",
    "    mask[::2] = 1\n",
    "    if orientation:\n",
    "      mask = 1 - mask # flip mask if orientation is True\n",
    "    return mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and data loader\n",
    "train_dataset = MNIST(root='./data', train=True, transform=ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NICE model\n",
    "model = NotSoNICE(input_dim=input_dim, num_coupling_layers=num_coupling_layers, num_layers=num_layers).to(device)\n",
    "\n",
    "model.train()\n",
    "for i, layer in enumerate(model.coupling_layers):\n",
    "    model.coupling_layers[i].mask = layer.mask.to(device)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Architecture: NotSoNICE\n",
    "- Optimizer: Adam\n",
    "- Loss: Negative log-likelihood (<b>Why?</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  mean_likelihood = 0.0\n",
    "  batch_counter = 0\n",
    "\n",
    "  for batch_id, (x, _) in enumerate(train_loader):\n",
    "      \n",
    "      model.zero_grad()\n",
    "\n",
    "      x = x.to(device)\n",
    "      x = x.view(-1, 28*28)  # flatten\n",
    "      \n",
    "      z, likelihood = model(x)\n",
    "      loss = -torch.mean(likelihood)  # Negative log-likelihood\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()      \n",
    "\n",
    "      mean_likelihood -= loss\n",
    "      batch_counter += 1\n",
    "\n",
    "  mean_likelihood /= batch_counter  # normalize w.r.t. the batches\n",
    "  print(f'Epoch {epoch+1:d} completed. Log Likelihood: {mean_likelihood:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.isdir(\"saved_models\"):\n",
    "#     os.makedirs(\"saved_models\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"saved_models/NotSoNICE.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
