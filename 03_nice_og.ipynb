{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.distributions import Uniform, Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CouplingLayer, ScalingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28  # input size (MNIST)\n",
    "hidden_dim = 1000  # output size of the hidden layers\n",
    "num_coupling_layers = 5  # number of coupling layers\n",
    "num_layers = 6  # number of linear layers for each coupling layer\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticDistribution(Distribution):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def log_prob(self, x):\n",
    "    return -(F.softplus(x) + F.softplus(-x))\n",
    "\n",
    "  def sample(self, size):\n",
    "    z = Uniform(torch.FloatTensor([0.]), torch.FloatTensor([1.])).sample(size)\n",
    "\n",
    "    return torch.log(z) - torch.log(1. - z)\n",
    "  \n",
    "class NICE(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim=1000, num_coupling_layers=3, num_layers=6, device='cpu'):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_coupling_layers = num_coupling_layers\n",
    "    self.num_layers = num_layers  # number of linear layers for each coupling layer\n",
    "\n",
    "    # alternating mask orientations for consecutive coupling layers\n",
    "    masks = [self._get_mask(input_dim, orientation=(i % 2 == 0)).to(device)\n",
    "                                            for i in range(num_coupling_layers)]\n",
    "\n",
    "    self.coupling_layers = nn.ModuleList([CouplingLayer(input_dim=input_dim,\n",
    "                                hidden_dim=hidden_dim,\n",
    "                                mask=masks[i], num_layers=num_layers)\n",
    "                              for i in range(num_coupling_layers)])\n",
    "\n",
    "    self.scaling_layer = ScalingLayer(input_dim=input_dim)\n",
    "\n",
    "    self.prior = LogisticDistribution()\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    z = x\n",
    "    for i in range(len(self.coupling_layers)):  # pass through each coupling layer\n",
    "      z = self.coupling_layers[i](z)\n",
    "    z, log_det_jacobian = self.scaling_layer(z)\n",
    "\n",
    "    log_likelihood = torch.sum(self.prior.log_prob(z), dim=1) + log_det_jacobian\n",
    "\n",
    "    return z, log_likelihood\n",
    "\n",
    "  def inverse(self, z):\n",
    "    x = z\n",
    "    x = self.scaling_layer.inverse(x)\n",
    "    for i in reversed(range(len(self.coupling_layers))):  # pass through each coupling layer in reversed order\n",
    "      x = self.coupling_layers[i].inverse(x)\n",
    "    return x\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    z = self.prior.sample([num_samples, self.input_dim]).view(num_samples, self.input_dim)\n",
    "    z = z.to(self.device)\n",
    "    return self.inverse(z)\n",
    "\n",
    "  def _get_mask(self, dim, orientation=True):\n",
    "    mask = torch.zeros(dim)\n",
    "    mask[::2] = 1.\n",
    "    if orientation:\n",
    "      mask = 1. - mask # flip mask if orientation is True\n",
    "    return mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and data loader\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NICE model\n",
    "model = NICE(input_dim=input_dim, num_coupling_layers=num_coupling_layers, num_layers=num_layers, device=device).to(device)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  tot_log_likelihood = 0\n",
    "  batch_counter = 0\n",
    "\n",
    "  for batch_id, (x, _) in tqdm(enumerate(train_loader)):\n",
    "      \n",
    "      model.zero_grad()\n",
    "\n",
    "      x = x.to(device)\n",
    "      x = x.view(-1, 28*28)  # flatten\n",
    "      \n",
    "      z, log_likelihood = model(x)\n",
    "      loss = -torch.mean(log_likelihood)  # NLL\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()      \n",
    "\n",
    "      tot_log_likelihood -= loss\n",
    "      batch_counter += 1\n",
    "\n",
    "  mean_log_likelihood = tot_log_likelihood / batch_counter  # normalize w.r.t. the batches\n",
    "  print(f'Epoch {epoch+1:d} completed. Log Likelihood: {mean_log_likelihood:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.isdir(\"saved_models\"):\n",
    "#     os.makedirs(\"saved_models\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"saved_models/NICE.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
